# utils.py
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms

from PIL import Image,ImageDraw, ImageFont
import numpy as np
import pandas as pd
import timm
import os

img_height, img_width = 512, 512  # 在数据加载阶段时，和最终输出到output 中的图像尺寸
img_max, img_min = 1., 0  # 图像像素值的最大值和最小值

# 论文中使用的模型
cnn_model_paper = ['resnet50', 'vgg16', 'mobilenet_v2', 'inception_v3']
vit_model_paper = ['vit_base_patch16_224', 'pit_b_224',
                   'visformer_small', 'swin_tiny_patch4_window7_224']

# 代码库中使用的模型
cnn_model_pkg = ['vgg19', 'resnet18', 'resnet101',
                 'resnext50_32x4d', 'densenet121', 'mobilenet_v2']
vit_model_pkg = ['vit_base_patch16_224', 'pit_b_224', 'cait_s24_224', 'visformer_small',
                 'tnt_s_patch16_224', 'levit_256', 'convit_base', 'swin_tiny_patch4_window7_224']

# 特定任务中使用的Vit模型
tgr_vit_model_list = ['vit_base_patch16_224', 'pit_b_224', 'cait_s24_224', 'visformer_small',
                      'deit_base_distilled_patch16_224', 'tnt_s_patch16_224', 'levit_256', 'convit_base']

# 仅在TTP类初始化生成器模型列表时被使用
generation_target_classes = [24, 99, 245, 344, 471, 555, 661, 701, 802, 919]

# 加载预训练的CNN和Vit模型
def load_pretrained_model(cnn_model=[], vit_model=[]):
    for model_name in cnn_model:
        yield model_name, models.__dict__[model_name](weights="DEFAULT")
        # yield model_name, models.__dict__[model_name](weights="IMAGENET1K_V1")
    for model_name in vit_model:
        yield model_name, timm.create_model(model_name, pretrained=True)


# 为模型添加预处理层，包括图像尺寸调整和归一化
def wrap_model(model):
    """
    Add normalization layer with mean and std in training configuration
    """
    model_name = model.__class__.__name__
    Resize = 224  # 数据预处理阶段，默认图像调整为244*244

    if hasattr(model, 'default_cfg'):
        """timm.models"""
        mean = model.default_cfg['mean']
        std = model.default_cfg['std']
    else:
        """torchvision.models"""
        if 'Inc' in model_name:  # 数据预处理阶段，如果模型名称包含Inc（例如 Inception 系列模型），则将 Resize 设置为 299
            mean = [0.5, 0.5, 0.5]
            std = [0.5, 0.5, 0.5]
            Resize = 299
        else:  # 数据预处理阶段，如果模型名称不包含 Inc，则保持 Resize 为默认的 224。
            mean = [0.485, 0.456, 0.406]
            std = [0.229, 0.224, 0.225]
            Resize = 224

    PreprocessModel = PreprocessingModel(Resize, mean, std)
    return torch.nn.Sequential(PreprocessModel, model)


# def save_images(output_dir, adversaries, filenames):
#     adversaries = (adversaries.detach().permute((0,2,3,1)).cpu().numpy() * 255).astype(np.uint8)
#     for i, filename in enumerate(filenames):
#         Image.fromarray(adversaries[i]).save(os.path.join(output_dir, filename))


# 将对抗样本保存为图像文件，并在图像底部添加攻击类型、原始标签、对抗标签、置信度的信息
def save_images(output_dir, adversaries, filenames, labels, adversarial_labels, targeted, imagenet_labels,confidences):
    # 检查输出目录是否存在，如果不存在则创建
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    adversaries = (adversaries.detach().permute((0, 2, 3, 1)).cpu().numpy() * 255).astype(np.uint8)
    for i, filename in enumerate(filenames):
        image = Image.fromarray(adversaries[i])

        # 扩展图片大小，在底部添加一块空白区域
        padding_height = 80  # 空白区域的高度
        new_image = Image.new('RGB', (image.width, image.height + padding_height), color=(255, 255, 255))
        new_image.paste(image, (0, 0))

        draw = ImageDraw.Draw(new_image)
        font = ImageFont.load_default()  # 可以根据需要更换字体

        attack_type = "Targeted" if targeted else "Untargeted"
        original_label_idx = labels[i].item()
        adversarial_label_idx = adversarial_labels[i].item()
        original_label = imagenet_labels[original_label_idx]
        adversarial_label = imagenet_labels[adversarial_label_idx]
        confidence = confidences[i].item()  # 获取当前图像的置信度分数

        # 构建换行显示的文本
        text = f"Attack Type: {attack_type}\n"
        text += f"Original Label: {original_label} ({original_label_idx})\n"
        text += f"Adversarial Label: {adversarial_label} ({adversarial_label_idx})\n"
        text += f"Confidence: {confidence:.2f}"  # 保留两位小数显示置信度分数

        draw.text((10, image.height + 10), text, fill=(0, 0, 0), font=font)  # 在空白区域添加文字

        new_image.save(os.path.join(output_dir, filename))

# 将张量 x 的值限制在 [x_min, x_max] 范围内。
def clamp(x, x_min, x_max):
    return torch.min(torch.max(x, x_min), x_max)


class PreprocessingModel(nn.Module):
    def __init__(self, resize, mean, std):
        super(PreprocessingModel, self).__init__()
        self.resize = transforms.Resize(resize)
        self.normalize = transforms.Normalize(mean, std)

    def forward(self, x):
        return self.normalize(self.resize(x))


class EnsembleModel(torch.nn.Module):
    def __init__(self, models, mode='mean'):
        super(EnsembleModel, self).__init__()
        self.device = next(models[0].parameters()).device
        for model in models:
            model.to(self.device)
        self.models = models
        self.softmax = torch.nn.Softmax(dim=1)
        self.type_name = 'ensemble'
        self.num_models = len(models)
        self.mode = mode

    def forward(self, x):
        outputs = []
        for model in self.models:
            outputs.append(model(x))
        outputs = torch.stack(outputs, dim=0)
        if self.mode == 'mean':
            outputs = torch.mean(outputs, dim=0)
            return outputs
        elif self.mode == 'ind':
            return outputs
        else:
            raise NotImplementedError


# 定义了一个自定义数据集类，用于加载对抗样本数据集
class AdvDataset(torch.utils.data.Dataset):
    def __init__(self, input_dir=None, output_dir=None, targeted=False, target_class=None, eval=False):
        self.targeted = targeted
        self.target_class = target_class
        self.data_dir = input_dir
        self.f2l = self.load_labels(os.path.join(self.data_dir, 'labels.csv'))

        if eval:
            self.data_dir = output_dir
            # load images from output_dir, labels from input_dir/labels.csv
            print('=> Eval mode: evaluating on {}'.format(self.data_dir))
        else:
            self.data_dir = os.path.join(self.data_dir, 'images')
            print('=> Train mode: training on {}'.format(self.data_dir))
            print('Save images to {}'.format(output_dir))

    def __len__(self):
        return len(self.f2l.keys())

    def __getitem__(self, idx):
        filename = list(self.f2l.keys())[idx]

        assert isinstance(filename, str)

        filepath = os.path.join(self.data_dir, filename)
        image = Image.open(filepath)
        image = image.resize((img_height, img_width)).convert('RGB')
        # Images for inception classifier are normalized to be in [-1, 1] interval.
        image = np.array(image).astype(np.float32)/255
        image = torch.from_numpy(image).permute(2, 0, 1)
        label = self.f2l[filename]

        return image, label, filename

    def load_labels(self, file_name):
        dev = pd.read_csv(file_name)
        if self.targeted:
            if self.target_class:
                f2l = {dev.iloc[i]['filename']: [dev.iloc[i]['label'], self.target_class] for i in range(len(dev))}
            else:
                f2l = {dev.iloc[i]['filename']: [dev.iloc[i]['label'],
                                             dev.iloc[i]['targeted_label']] for i in range(len(dev))}
        else:
            f2l = {dev.iloc[i]['filename']: dev.iloc[i]['label']
                   for i in range(len(dev))}
        return f2l


if __name__ == '__main__':
    dataset = AdvDataset(input_dir='./data_targeted',
                         targeted=True, eval=False)

    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=4, shuffle=False, num_workers=0)

    for i, (images, labels, filenames) in enumerate(dataloader):
        print(images.shape)
        print(labels)
        print(filenames)
        break
